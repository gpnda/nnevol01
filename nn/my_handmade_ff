# -*- coding: utf-8 -*-

# Использование модуля nn_module.py
# Подключаем модуль
# Создаем экземпляр класса нейронной сети, с помощью конструктора nn(n_neurons_pl, n_inputs)
# n_neurons_pl - одномерный массив, содержащий количество нейронов в каждом слое
# Последний элемент определяет сколько будет у сети выходов
# n_inputs - число входов сети (просто целое положительное число) (а значит количество синапсов у нейронов первого слоя)
# количество синапсов у нейронов каждого следующего слоя равно количеству нейронов в предыдущем слое
# Эксплуатировать экземпляр класса нужно таким образом:
# Заполняем массив входов созданной сети:
# mynn.n_inputs=[1,1,1,1]
# рассчитываем сеть
# mynn.calc()
# выводим на экран выход сети
# print(mynn._outs)
# что-то есть неправильное в том что входы первого слоя нейронов -(вход сети) - дискретные: 0...1,
# а входы нейронов находящихся внутри сети - вещественнные числа -1.0 ... +1.0


import math  # подключить бибилиотеку математических функций. Обращение через math.XXX
import random  # подключить бибилиотеку случайных чисел Обращение через random.XXX


def myrandom(lower=0.0, upper=1.0):  # Объявляем собственную Функцию случайных чисел
    x = random.random()  # получить нормализованное случайное число
    return lower + x * (upper - lower)  # преобразуем его к требуемому промежутку


class neuron:  # объявляем класс neuron
    def __init__(self, n_sinaps):  # определяем функцию -  конструктор
        self._weights = []  # параметр класса neuron - _weights  это список с весами, изначально - пустой, без элементов списка
        for i in range(
                n_sinaps + 1):  # в цикле по всем синапсам+1 (n_sinaps - глобальная переменная приходит снаружи класса)
            self._weights.append(myrandom(-10.005, 10.005))  # добавляем в список по одному - случайные веса из промежутка -0.005 до 0.005, такие маленькие числа, чтобы избежать паралича сети
            # self._weights.append(0.1)

            # паралич сети это явление очевидно свзяанное с процедурой обучения с учителем методом обратного распространения где учитывается производная от активационной функции, и когда веса уходят в
            # большие величины, в ту область сигмоиды, где производная практически равняется нулю, обучение становится невозможным
            # в нашем случае обучение стохастическим методом, не связано с наклоном сигмоиды, можно допустить что паралич сети не грозит в нашем случае, так что можно задать более широкий диапазон стартовых сначений весов.
        ## One weight (last one) - adjusting of sigmoid
        self._out = 0.0  # определяем и обнуляем выход нейрона, он является вещественным числом, потому что это фактически - результат расчета сигмоиды, а она возвращает результат в промежутке (0..1)
        self._inputs = []  # определяем и обнуляем список входов, ожидается что входы - дискретны 0/1. На маомент создания экземпляра класса значения этих входов пока неизвестны, но они будут известны в момент расчета функции calc()

    def FnActivation(self, s=0.0):  # активационная функция
        return 1 / (1 + math.exp(-s))  # классическая сигмоида, по крайней мере так задумывалась.

    def calc(self):  # функция расчета выхода
        tmpinputs = self._inputs[0:len(
            self._inputs)]  # назначаем локальный массив ВХОДОВ, копируя его из входов текущего экземпляра класса neuron
        tmpinputs.append(
            1)  ## One input (last one) always =1 for adjusting of sigmoid # Да один вход, последний в списке входов - всегда включен чтобы имитировать вертикальный сдвиг активационной функции. также как и в весах последний вес - дополнительный.
        ## print "tmpinputs"+str(tmpinputs)
        ## print "self._weights"+str(self._weights)
        s = sum(map(lambda v1, v2: v1 * v2, tmpinputs,
                    self._weights))  # объявлена лямбда функция, умножающая две переменных, далее два массива ВХОДЫ_локальные_для_функции_calc и ВЕСА_параметр_объекта_neuron были промапированы по этой лямбде-функции, и в конце все просуммировано, хитрая строчка
        self._out = self.FnActivation(
            s)  # расчитан выход = активационная функция от суммы, расчитанной в предыдущей строке
        # print("Выход нейрона: " + str(self._out))
        return self._out  # возвращаем результат работы функции calc

    def __str__(self):  # заводим функцию, которая создает строковое представление объекта neuron
        return str(self._weights)  # по сути она просто печатает массив весов для синапсов данного нейрона.


class layer:  # объявляем класс слоя
    def __init__(self, n_neurons, n_inputs):  # это конструктор класса слоя, в качестве входных параметров принимает:
        # n_neurons - число_нейронов в слое  и
        # n_inputs - число_входов слоя .
        self._neurons = []  # он объявляет массив нейронов данного слоя, пока пустой. этот массив будет являться параметром слоя
        for i in range(n_neurons):  # заводим цикл с количеством итераций равным объявленному числу нейронов в слое
            self._neurons.append(neuron(
                n_inputs))  # заполняем массив нейронов собственно новенькими нейронами, количество входов каждого нейрона выставляется в соответствии с количеством входов слоя n_inputs
        self._outs = []  # объявляем и обнуляем массив выходов, он пока пустой но после расчета слоя в нем будет ровно столько вещественных чисел, сколько нейронов в данном слое
        self._inputs = []  # объявляем и обнуляем массив входов слоя. На момент создания экземпляра класса слоя, значения этих входов неизвестны но мы достоверно знаем их количество, так как конструктор получает его явным образом в качестве аргумента.

    def calc(self):  # объявляем функцию расчета слоя
        self._outs = []  # обнуляем выходы - КОСТЫЛЬ, потому что бессмысленно на данном этапе, просто возможно после предыдущего шага выходы не обнулились.
        for i in self._neurons:  # Прогоняем цикл по всему массиву нейронов текущего слоя
            i._inputs = self._inputs  # Назначаем входы данного нейрона = входы текущего слоя
            # ?????????????  Нет ли тут ошибки? может быть надо както по-извращенски назначать, например созданием независимой копии?
            # i._inputs = self._inputs[0:len(self._inputs)] ## собственно - вот вариант приравнивания который создает копию. Этот вариант на случай нежелательного склеивания массивов
            self._outs.append(
                i.calc())  # и расчитываем собственно нейрон. На этот момент веса известны, либо назначаны в конструкторе, если это первая итерация, либо просто текущий экземпляр слоя помнит веса каждого нейрона в отдельности.
        # ?????????????  хорошо бы удостовериться что на каждом шаге слой помнит все веса всех нейронов и если идет обучение - изменение весов, то на следующем шаге веса измпеняются и запоминаются измененными.

    def __str__(self):  # заводим строковое представление текущего слоя.
        return str(self._outs)  # в данном случае мы просто печатаем массив выходов.


class nn:  # класс нейронной сети
    def __init__(self, n_neurons_pl, n_inputs):  # конструктор класса, требует указания параметров:
        ## n_neurons_pl - Количество нейронов в каждом слое (это массив из целых чисел,
        ##	  и последний слой определяет количество выходов)
        ##        количество элементов массива определяет количество слоев в сети
        ## n_inputs - Количество входов сети
        self.n_layers = len(n_neurons_pl)  # вычисляем количество слоев в сети, исходя из размера массива n_neurons_pl
        self._layers = []  # заводим параметр массив слоев, пока пустой.
        self._layers.append(layer(n_neurons_pl[0],
                                  n_inputs))  # инициализируем/создаем нулевой слой, определяем ему количество входов равным количеству входов всей сети.
        for i in range(1, self.n_layers):  # далее в цикле инициализируем/создаем остальные слои, кроме нулевого
            ## ++++???????????  Хорошо бы проверить что этот цикл действительно игнорирует нулевой (первый) элемент массива
            ## Проверено, действительно игнорирует 17.10.2011 файл test_first_element_of_array.py
            self._layers.append(layer(n_neurons_pl[i], n_neurons_pl[
                i - 1]))  # назначаем им количество входов  равным количеству нейронов в предыдущем слое.
        self._outs = []  # заводим массив выходов сети, пока пустой, количество элементов в этом масиве мы на данный момент знаем: это количество нейронов в последнем слое
        self._inputs = []  # заводим массив входов сети, пока пустой, количество элементов в этом массиве мы на данный момент знаем: это количество входов сети.

    def calc(self):  # функция расчета сети
        self._layers[0]._inputs = self._inputs  # назначаем массив входов для нулевого слоя сети
        # ??????????????  Нет ли тут ошибки? может быть надо както по-извращенски назначать, например созданием независимой копии?
        # self._layers[0]._inputs = self._inputs[0:len(self._inputs)] # собственно - вот вариант приравнивания который создает копию. Этот вариант на случай нежелательного склеивания массивов
        ## print "Layer 0  Ins: " + str(self._layers[0]._inputs)
        self._layers[0].calc()  # расчитать нулевой слой
        ## print "Layer 0 Outs: " + str(self._layers[0]._outs)
        for i in range(1, self.n_layers):  # В цикле по всем слоям кроме нулевого
            self._layers[i]._inputs = self._layers[
                i - 1]._outs  # назначаем входы текущего слоя = выходы предыдущего слоя
            # ????????????  Нет ли тут ошибки? может быть надо както по-извращенски назначать, например созданием независимой копии?
            # self._layers[i]._inputs = self._layers[i-1]._outs[0:len(self._layers[i-1]._outs)]  ## Этот вариант на случай нежелательного склеивания массивов
            ## print "Layer " + str(i) + "  Ins: " + str(self._layers[i]._inputs)
            self._layers[i].calc()  # расчитываем i-й слой
        ## print "Layer " + str(i) + " Outs: " + str(self._layers[i]._outs)
        self._outs = self._layers[len(self._layers) - 1]._outs  # назначаем выходы сети = выходы последнего слоя
        ## ++++++?????????? точно это выходы последнего слоя?
        ## Теоретически да, проверено 17.10.11 файл test_first_element_of_array.py
        ## Надо еще проверить на реальной сети

    def __strOld__(self):  # заводим строковое представление для нейронной сети
        s = "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n"  # просто выводим все веса, всех слоев
        s += "* one weight (last) for adjusting sigmoid\n"
        for i in self._layers:
            for j in i._neurons:
                s += "	   Weights (amount=" + str(len(j._weights)) + "):" + str(j._weights) + "\n"
        s += "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n"
        return str(s)

    def __str__(self):  # заводим строковое представление для нейронной сети
        s = "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n"  # просто выводим все веса, всех слоев
        # s += "* one weight (last) for adjusting sigmoid\n"
        for i in self._layers:
            s += "LAYER START:\n"
            for j in i._neurons:
                s += "Neuron start:\n"
                for w in j._weights:
                    s += str(w) + "\t"
                s += "\n"
        s += "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n"
        return str(s)
